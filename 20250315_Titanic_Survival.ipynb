{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93eca0eb-906a-4d90-b9fa-af91183c5bd7",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Titanic Survival Prediction Portfolio Project\n",
    "Author: YI LUO\n",
    "Date: 20250315\n",
    "\n",
    "Introduction:\n",
    "This portfolio project demonstrates an end-to-end data analysis and machine learning workflow \n",
    "using the Titanic dataset from Kaggle (https://www.kaggle.com/competitions/titanic/data). \n",
    "The main objective is to predict whether a passenger survived the Titanic disaster.\n",
    "\n",
    "Key steps and findings:\n",
    "1. Data Preprocessing:\n",
    "   - Handled missing values: Age filled with median values (grouped by Pclass and Sex), Fare with median, and Embarked with mode.\n",
    "   - Converted categorical variables such as Sex (male=1, female=0) and Embarked into one-hot encoded features.\n",
    "   - Extracted new features from raw data: \n",
    "     • Title: Extracted from Name to capture social status and gender cues.\n",
    "     • Deck: Derived from the first character of Ticket to indicate the deck level.\n",
    "     • FamilySize: Constructed as a weighted combination of SibSp and Parch, then log-transformed for smoothing.\n",
    "     • FareBin and AgeBin: Discretized versions of Fare and Age to capture non-linear effects.\n",
    "   \n",
    "2. Exploratory Data Analysis (EDA):\n",
    "   - Visualized distributions of key features (e.g., AgeBin, FareBin) and examined survival rates across different passenger classes.\n",
    "   - Insights revealed that gender, class, and title are strong predictors of survival.\n",
    "\n",
    "3. Model Building:\n",
    "   - Built a 6-layer Deep Neural Network (DNN) using TensorFlow/Keras.\n",
    "   - Integrated BatchNormalization and Dropout layers to stabilize training and prevent overfitting.\n",
    "   - Employed EarlyStopping and ModelCheckpoint callbacks to capture the best model.\n",
    "   - The model was trained using normalized data and an 80/20 train-validation split.\n",
    "\n",
    "4. Evaluation and Submission:\n",
    "   - The DNN achieved competitive validation accuracy (around 81.5% on the validation set).\n",
    "   - Final predictions on the test set were generated and formatted as per Kaggle requirements for submission.\n",
    "\n",
    "This project highlights a comprehensive data science approach—from data cleaning and feature engineering to deep learning model training—demonstrating my ability to tackle real-world predictive modeling challenges. This work is an integral part of my portfolio, showcasing the techniques and insights that I can bring to a data science role.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae11658-d05d-496f-b51c-d6f86eb9e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the training and test data\n",
    "df = pd.read_csv(\"/Users/luoyi/Desktop/10_kaggle/01_titanic/train.csv\")\n",
    "test_df = pd.read_csv(\"/Users/luoyi/Desktop/10_kaggle/01_titanic/test.csv\")\n",
    "\n",
    "# Save PassengerId for final submission\n",
    "test_passenger_ids = test_df['PassengerId']\n",
    "\n",
    "# Quick overview of the training data\n",
    "print(\"Training data shape:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2468ed9-40ad-4558-bc07-7a4ba4844dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Fill Missing Values\n",
    "# ---------------------------\n",
    "# Fill missing Age values using median grouped by Pclass and Sex\n",
    "df['Age'] = df.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "test_df['Age'] = test_df.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Fill missing Fare in test data with median\n",
    "test_df['Fare'].fillna(test_df['Fare'].median(), inplace=True)\n",
    "\n",
    "# Fill missing Embarked values with the mode (most common value)\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "test_df['Embarked'].fillna(test_df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Feature Extraction & Transformation\n",
    "# ---------------------------\n",
    "# 2.1 Extract Title from Name and map rare titles\n",
    "df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "test_df['Title'] = test_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',\n",
    "    'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare',\n",
    "    'Jonkheer': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs',\n",
    "    'Capt': 'Rare', 'Sir': 'Rare'\n",
    "}\n",
    "df['Title'] = df['Title'].map(title_mapping)\n",
    "test_df['Title'] = test_df['Title'].map(title_mapping)\n",
    "\n",
    "# One-Hot encode Title for both train and test data\n",
    "df = pd.get_dummies(df, columns=['Title'], drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=['Title'], drop_first=True)\n",
    "\n",
    "# 2.2 Convert Sex to numeric (male: 1, female: 0)\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "test_df['Sex'] = test_df['Sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "# 2.3 Extract Deck from Ticket\n",
    "df['Deck'] = df['Ticket'].apply(lambda x: str(x)[0])\n",
    "test_df['Deck'] = test_df['Ticket'].apply(lambda x: str(x)[0])\n",
    "df['Deck'] = df['Deck'].apply(lambda x: x if x.isalpha() else 'X')\n",
    "test_df['Deck'] = test_df['Deck'].apply(lambda x: x if x.isalpha() else 'X')\n",
    "df = pd.get_dummies(df, columns=['Deck'], drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=['Deck'], drop_first=True)\n",
    "\n",
    "# 2.4 Create FamilySize feature (using a weighted formula) and apply a log transform\n",
    "df['FamilySize'] = np.log1p(df['SibSp'] * 3 + df['Parch'] * 2 + 1)\n",
    "test_df['FamilySize'] = np.log1p(test_df['SibSp'] * 3 + test_df['Parch'] * 2 + 1)\n",
    "\n",
    "# 2.5 One-Hot encode Embarked\n",
    "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=['Embarked'], drop_first=True)\n",
    "\n",
    "# 2.6 Create FareBin: Discretize Fare into 5 quantile bins\n",
    "df['FareBin'] = pd.qcut(df['Fare'], 5, labels=[1, 2, 3, 4, 5]).astype(int)\n",
    "test_df['FareBin'] = pd.qcut(test_df['Fare'], 5, labels=[1, 2, 3, 4, 5]).astype(int)\n",
    "\n",
    "# 2.7 Create AgeBin: Discretize Age into 5 bins\n",
    "df['AgeBin'] = pd.cut(df['Age'], bins=[0, 12, 20, 40, 60, 100], labels=[1, 2, 3, 4, 5]).astype(int)\n",
    "test_df['AgeBin'] = pd.cut(test_df['Age'], bins=[0, 12, 20, 40, 60, 100], labels=[1, 2, 3, 4, 5]).astype(int)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Drop Unnecessary Columns\n",
    "# ---------------------------\n",
    "# We drop columns that are not useful for prediction:\n",
    "# - Name: already used to extract Title\n",
    "# - Ticket: used for Deck extraction\n",
    "# - Cabin: too many missing values\n",
    "# - PassengerId: not predictive\n",
    "# - SibSp, Parch, Fare, Age: as we have new features and bins for these\n",
    "drop_cols = ['Name', 'Ticket', 'Cabin', 'PassengerId', 'SibSp', 'Parch', 'Fare', 'Age']\n",
    "df.drop(columns=drop_cols, inplace=True)\n",
    "test_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Ensure Consistency between Train and Test\n",
    "# ---------------------------\n",
    "missing_cols = set(df.columns) - set(test_df.columns)\n",
    "for col in missing_cols:\n",
    "    test_df[col] = 0  # Fill missing columns in test set with 0\n",
    "# Align test_df column order with train set (excluding Survived)\n",
    "test_df = test_df[df.columns.drop('Survived')]\n",
    "\n",
    "# Check for missing values in the processed data\n",
    "print(\"Train missing values:\\n\", df.drop(columns=['Survived']).isnull().sum())\n",
    "print(\"Test missing values:\\n\", test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b50a7-ae94-4433-9a2b-be092239b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot distribution of AgeBin and FareBin\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='AgeBin', data=df)\n",
    "plt.title(\"Distribution of Age Bins\")\n",
    "plt.xlabel(\"Age Bin\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='FareBin', data=df)\n",
    "plt.title(\"Distribution of Fare Bins\")\n",
    "plt.xlabel(\"Fare Bin\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot survival rate by Pclass\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x='Pclass', y='Survived', data=df)\n",
    "plt.title(\"Survival Rate by Pclass\")\n",
    "plt.ylabel(\"Survival Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e4d96-5079-458a-8444-aa49afc2b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['Survived'])\n",
    "y = df['Survived']\n",
    "\n",
    "# Apply standard scaling (DNN performs better with normalized data)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(test_df)\n",
    "\n",
    "# Split the training data into train and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db62324-a430-4a0b-9e1c-344d937711a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Build a 6-layer DNN model\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(X_train.shape[1],)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Define optimizer with a custom learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d3589-6058-4a10-8266-e7d50ac1a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks: EarlyStopping to prevent overfitting, and ModelCheckpoint to save the best model\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint('best_dnn_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=16,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model on training and validation sets\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38156232-7114-4349-9314-f320ca19e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "test_preds = model.predict(X_test_scaled)\n",
    "# Convert probabilities to binary 0/1 outcomes\n",
    "test_preds = (test_preds > 0.5).astype(int)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Survived': test_preds.flatten()\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('/Users/luoyi/Desktop/10_kaggle/01_titanic/submission_dnn.csv', index=False)\n",
    "print(\"Submission file generated!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369e8aa-6513-4af2-9b5a-e37037a3fcec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
