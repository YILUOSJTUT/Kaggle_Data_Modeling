{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c42b763-2f85-494a-ba02-440e01d36152",
   "metadata": {},
   "source": [
    "This portfolio presents a data analysis project for the Geology Forecast Challenge, aiming to predict 1D layer-depth sequences from high-dimensional geophysical measurements using the publicly available Kaggle dataset ([https://www.kaggle.com/competitions/geology-forecast-challenge-open/data](https://www.kaggle.com/competitions/geology-forecast-challenge-open/data)).\n",
    "\n",
    "Key steps:\n",
    "\n",
    "1. **Data Preparation**\n",
    "   ‚Ä¢ Loaded 3,300 feature columns (measurements at depths ‚àí299 to 300 across ten ‚Äúrealizations‚Äù).\n",
    "   ‚Ä¢ Imputed missing values with column means, applied a log-transform (`log(31 + x)`), and standardized all features.\n",
    "   ‚Ä¢ Extracted 3,000-dimensional targets per sample and standardized them for numerical stability.\n",
    "\n",
    "2. **Modeling**\n",
    "   ‚Ä¢ Built a six-layer PyTorch DNN: 3,300 ‚Üí 1,024 ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 3,000, using BatchNorm, GELU, and dropout to balance expressiveness and regularization.\n",
    "   ‚Ä¢ Trained with 5-fold cross-validation, Adam optimizer (LR=1e-3, weight decay=1e-5), ReduceLROnPlateau, and early stopping (patience=5).\n",
    "   ‚Ä¢ Collected out-of-fold (OOF) predictions to compute overall RMSE on the original scale.\n",
    "\n",
    "3. **Ensembling & Post-Processing**\n",
    "   ‚Ä¢ Averaged test predictions across all folds, inverted target scaling, and applied an ‚Äúaverage-trick‚Äù: for each of the 300 depths, averaged the ten realization channels and repeated that mean to stabilize outputs.\n",
    "\n",
    "**Key finding:** The deep, progressively compressing network plus careful regularization and post-processing outperformed simpler baselines, yielding improved OOF RMSE and a competitive leaderboard position.\n",
    "\n",
    "¬†¬†¬†\n",
    "**@author**  YI LUO\n",
    "**@date**  20250601\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f88d746-fe32-40b6-a956-3cd4f2cbad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12035c6b-8a1b-4f30-abe4-f5e6ab5da366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Load Data\n",
    "TRAIN_PATH = './00_data/train.csv'\n",
    "TEST_PATH  = './00_data/test.csv'\n",
    "SUB_PATH   = './00_data/sample_submission.csv'\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sub   = pd.read_csv(SUB_PATH)\n",
    "\n",
    "FEATURES = [c for c in test.columns if c != 'geology_id']\n",
    "TARGETS  = [c for c in sub.columns if c != 'geology_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51106bf-da9c-4c49-b185-3273d7df51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Preprocessing (Impute + Log + Scale)\n",
    "train_feats = train[FEATURES].copy()\n",
    "test_feats  = test[FEATURES].copy()\n",
    "\n",
    "train_feats = train_feats.fillna(train_feats.mean())\n",
    "test_feats  = test_feats.fillna(test_feats.mean())\n",
    "\n",
    "X_raw      = np.log(31.0 + train_feats.values)\n",
    "X_test_raw = np.log(31.0 + test_feats.values)\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "X_train = feature_scaler.fit_transform(X_raw)\n",
    "X_test  = feature_scaler.transform(X_test_raw)\n",
    "\n",
    "y_raw = train[TARGETS].copy().values.astype(np.float32)\n",
    "target_scaler = StandardScaler()\n",
    "y_scaled = target_scaler.fit_transform(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97cfe20e-7da5-4791-a39b-bd674d6d1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Dataset / DataLoader Helper\n",
    "def create_dataloader(X_data, y_data=None, batch_size=128, shuffle=False):\n",
    "    if y_data is not None:\n",
    "        ds = TensorDataset(\n",
    "            torch.tensor(X_data, dtype=torch.float32),\n",
    "            torch.tensor(y_data, dtype=torch.float32)\n",
    "        )\n",
    "    else:\n",
    "        ds = TensorDataset(torch.tensor(X_data, dtype=torch.float32))\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45495bf7-5e59-4661-bbb3-756e418bbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: DNN Model Definition\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d9f906b-5de8-47df-993a-78ad924826fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üïê Fold 1/5\n",
      "Epoch 01 | Train Loss 0.928717 | Val Loss 0.770148\n",
      "Epoch 02 | Train Loss 0.553296 | Val Loss 0.453052\n",
      "Epoch 03 | Train Loss 0.341373 | Val Loss 0.258033\n",
      "Epoch 04 | Train Loss 0.237070 | Val Loss 0.223953\n",
      "Epoch 05 | Train Loss 0.230055 | Val Loss 0.274731\n",
      "Epoch 06 | Train Loss 0.214801 | Val Loss 0.239698\n",
      "Epoch 07 | Train Loss 0.218853 | Val Loss 0.193959\n",
      "Epoch 08 | Train Loss 0.204219 | Val Loss 0.199415\n",
      "Epoch 09 | Train Loss 0.200646 | Val Loss 0.251565\n",
      "Epoch 10 | Train Loss 0.199135 | Val Loss 0.186487\n",
      "Epoch 11 | Train Loss 0.180643 | Val Loss 0.198261\n",
      "Epoch 12 | Train Loss 0.184164 | Val Loss 0.173220\n",
      "Epoch 13 | Train Loss 0.179829 | Val Loss 0.183646\n",
      "Epoch 14 | Train Loss 0.208639 | Val Loss 0.193868\n",
      "Epoch 15 | Train Loss 0.195149 | Val Loss 0.181759\n",
      "Epoch 16 | Train Loss 0.180730 | Val Loss 0.181050\n",
      "Epoch 17 | Train Loss 0.192935 | Val Loss 0.178563\n",
      "‚èπ Early stopping triggered.\n",
      "\n",
      "üïê Fold 2/5\n",
      "Epoch 01 | Train Loss 0.947533 | Val Loss 0.699316\n",
      "Epoch 02 | Train Loss 0.596679 | Val Loss 0.416651\n",
      "Epoch 03 | Train Loss 0.339868 | Val Loss 0.304080\n",
      "Epoch 04 | Train Loss 0.242854 | Val Loss 0.368079\n",
      "Epoch 05 | Train Loss 0.209455 | Val Loss 0.236538\n",
      "Epoch 06 | Train Loss 0.177750 | Val Loss 0.253401\n",
      "Epoch 07 | Train Loss 0.177427 | Val Loss 0.243987\n",
      "Epoch 08 | Train Loss 0.174546 | Val Loss 0.233345\n",
      "Epoch 09 | Train Loss 0.167903 | Val Loss 0.226298\n",
      "Epoch 10 | Train Loss 0.177393 | Val Loss 0.249796\n",
      "Epoch 11 | Train Loss 0.190186 | Val Loss 0.228455\n",
      "Epoch 12 | Train Loss 0.188504 | Val Loss 0.245729\n",
      "Epoch 13 | Train Loss 0.183051 | Val Loss 0.234147\n",
      "Epoch 14 | Train Loss 0.168894 | Val Loss 0.229023\n",
      "‚èπ Early stopping triggered.\n",
      "\n",
      "üïê Fold 3/5\n",
      "Epoch 01 | Train Loss 0.926905 | Val Loss 0.762804\n",
      "Epoch 02 | Train Loss 0.579125 | Val Loss 0.414702\n",
      "Epoch 03 | Train Loss 0.326505 | Val Loss 0.270656\n",
      "Epoch 04 | Train Loss 0.237036 | Val Loss 0.210243\n",
      "Epoch 05 | Train Loss 0.204743 | Val Loss 0.243252\n",
      "Epoch 06 | Train Loss 0.200424 | Val Loss 0.193048\n",
      "Epoch 07 | Train Loss 0.203003 | Val Loss 0.214819\n",
      "Epoch 08 | Train Loss 0.200652 | Val Loss 0.200665\n",
      "Epoch 09 | Train Loss 0.190711 | Val Loss 0.233858\n",
      "Epoch 10 | Train Loss 0.192541 | Val Loss 0.198743\n",
      "Epoch 11 | Train Loss 0.186057 | Val Loss 0.204250\n",
      "‚èπ Early stopping triggered.\n",
      "\n",
      "üïê Fold 4/5\n",
      "Epoch 01 | Train Loss 0.940084 | Val Loss 0.769061\n",
      "Epoch 02 | Train Loss 0.577328 | Val Loss 0.428105\n",
      "Epoch 03 | Train Loss 0.331514 | Val Loss 0.254010\n",
      "Epoch 04 | Train Loss 0.236022 | Val Loss 0.245451\n",
      "Epoch 05 | Train Loss 0.234109 | Val Loss 0.273089\n",
      "Epoch 06 | Train Loss 0.202228 | Val Loss 0.208687\n",
      "Epoch 07 | Train Loss 0.189022 | Val Loss 0.207550\n",
      "Epoch 08 | Train Loss 0.180158 | Val Loss 0.194578\n",
      "Epoch 09 | Train Loss 0.168445 | Val Loss 0.238280\n",
      "Epoch 10 | Train Loss 0.179818 | Val Loss 0.223391\n",
      "Epoch 11 | Train Loss 0.172273 | Val Loss 0.192621\n",
      "Epoch 12 | Train Loss 0.167956 | Val Loss 0.196519\n",
      "Epoch 13 | Train Loss 0.200849 | Val Loss 0.237391\n",
      "Epoch 14 | Train Loss 0.198343 | Val Loss 0.193652\n",
      "Epoch 15 | Train Loss 0.171297 | Val Loss 0.197609\n",
      "Epoch 16 | Train Loss 0.166665 | Val Loss 0.193831\n",
      "‚èπ Early stopping triggered.\n",
      "\n",
      "üïê Fold 5/5\n",
      "Epoch 01 | Train Loss 0.923823 | Val Loss 0.815202\n",
      "Epoch 02 | Train Loss 0.589150 | Val Loss 0.420975\n",
      "Epoch 03 | Train Loss 0.326996 | Val Loss 0.219960\n",
      "Epoch 04 | Train Loss 0.236135 | Val Loss 0.229305\n",
      "Epoch 05 | Train Loss 0.210226 | Val Loss 0.188978\n",
      "Epoch 06 | Train Loss 0.191583 | Val Loss 0.192712\n",
      "Epoch 07 | Train Loss 0.190200 | Val Loss 0.162012\n",
      "Epoch 08 | Train Loss 0.184952 | Val Loss 0.168755\n",
      "Epoch 09 | Train Loss 0.195270 | Val Loss 0.167585\n",
      "Epoch 10 | Train Loss 0.193747 | Val Loss 0.204387\n",
      "Epoch 11 | Train Loss 0.178241 | Val Loss 0.170388\n",
      "Epoch 12 | Train Loss 0.176417 | Val Loss 0.167863\n",
      "‚èπ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# Section 5: Training Loop with K-Fold CV, Early Stopping, LR Scheduler\n",
    "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FOLDS        = 5\n",
    "EPOCHS       = 50\n",
    "BATCH_SIZE   = 128\n",
    "LR           = 1e-3\n",
    "PATIENCE     = 5\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_test  = X_test.shape[0]\n",
    "n_tgt   = len(TARGETS)\n",
    "\n",
    "test_preds = np.zeros((n_test, n_tgt), dtype=np.float32)\n",
    "oof_preds  = np.zeros((n_train, n_tgt), dtype=np.float32)\n",
    "\n",
    "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train), start=1):\n",
    "    print(f\"\\nüïê Fold {fold}/{FOLDS}\")\n",
    "\n",
    "    X_tr, y_tr = X_train[train_idx], y_scaled[train_idx]\n",
    "    X_val, y_val = X_train[val_idx], y_scaled[val_idx]\n",
    "\n",
    "    train_loader = create_dataloader(X_tr, y_tr, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = create_dataloader(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader  = create_dataloader(X_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model     = DNN(input_dim=X_train.shape[1], output_dim=n_tgt).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=2, factor=0.5\n",
    "    )\n",
    "    loss_fn   = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "        model.eval()\n",
    "        val_preds_fold = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                pred = model(xb).cpu().numpy()\n",
    "                val_preds_fold.append(pred)\n",
    "        val_preds_fold = np.vstack(val_preds_fold)\n",
    "        val_loss = mean_squared_error(y_val, val_preds_fold)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Train Loss {avg_train_loss:.6f} | Val Loss {val_loss:.6f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(\"‚èπ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        fold_val_preds = []\n",
    "        for xb, _ in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb).cpu().numpy()\n",
    "            fold_val_preds.append(pred)\n",
    "        fold_val_preds = np.vstack(fold_val_preds)\n",
    "        oof_preds[val_idx, :] = fold_val_preds\n",
    "\n",
    "    model.eval()\n",
    "    fold_test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb).cpu().numpy()\n",
    "            fold_test_preds.append(pred)\n",
    "        fold_test_preds = np.vstack(fold_test_preds)\n",
    "        test_preds += fold_test_preds / FOLDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469cc070-40a6-4819-bbda-7becdc4b8cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF RMSE (raw): 2.860820\n"
     ]
    }
   ],
   "source": [
    "# Section 6: Inverse Transform & Prepare Submission DataFrame\n",
    "oof_preds_inv  = target_scaler.inverse_transform(oof_preds)\n",
    "test_preds_inv = target_scaler.inverse_transform(test_preds)\n",
    "\n",
    "oof_score = mean_squared_error(train[TARGETS].values, oof_preds_inv)\n",
    "print(f\"OOF RMSE (raw): {np.sqrt(oof_score):.6f}\")\n",
    "\n",
    "sub_df = pd.DataFrame(test_preds_inv, columns=TARGETS)\n",
    "sub_df.insert(0, 'geology_id', test['geology_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc9fe9ab-558a-4979-8754-189072687cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geology_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>r_9_pos_291</th>\n",
       "      <th>r_9_pos_292</th>\n",
       "      <th>r_9_pos_293</th>\n",
       "      <th>r_9_pos_294</th>\n",
       "      <th>r_9_pos_295</th>\n",
       "      <th>r_9_pos_296</th>\n",
       "      <th>r_9_pos_297</th>\n",
       "      <th>r_9_pos_298</th>\n",
       "      <th>r_9_pos_299</th>\n",
       "      <th>r_9_pos_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g_4a52df537a</td>\n",
       "      <td>-0.006917</td>\n",
       "      <td>-0.013030</td>\n",
       "      <td>-0.019454</td>\n",
       "      <td>-0.027568</td>\n",
       "      <td>-0.032346</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.046534</td>\n",
       "      <td>-0.056739</td>\n",
       "      <td>-0.058770</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.441678</td>\n",
       "      <td>-1.524261</td>\n",
       "      <td>-1.582406</td>\n",
       "      <td>-1.557901</td>\n",
       "      <td>-1.414614</td>\n",
       "      <td>-1.386626</td>\n",
       "      <td>-1.417733</td>\n",
       "      <td>-1.498469</td>\n",
       "      <td>-1.480004</td>\n",
       "      <td>-1.477566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g_1e4b5a1509</td>\n",
       "      <td>0.013678</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.042944</td>\n",
       "      <td>0.054485</td>\n",
       "      <td>0.070491</td>\n",
       "      <td>0.083501</td>\n",
       "      <td>0.097020</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.125039</td>\n",
       "      <td>...</td>\n",
       "      <td>3.692461</td>\n",
       "      <td>3.666321</td>\n",
       "      <td>3.558129</td>\n",
       "      <td>3.647701</td>\n",
       "      <td>3.779645</td>\n",
       "      <td>3.833497</td>\n",
       "      <td>3.800299</td>\n",
       "      <td>3.756183</td>\n",
       "      <td>3.747805</td>\n",
       "      <td>3.832135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g_5919c0bea3</td>\n",
       "      <td>0.028208</td>\n",
       "      <td>0.056853</td>\n",
       "      <td>0.085918</td>\n",
       "      <td>0.112302</td>\n",
       "      <td>0.142245</td>\n",
       "      <td>0.169681</td>\n",
       "      <td>0.198137</td>\n",
       "      <td>0.224085</td>\n",
       "      <td>0.254682</td>\n",
       "      <td>...</td>\n",
       "      <td>7.240481</td>\n",
       "      <td>7.264407</td>\n",
       "      <td>7.187561</td>\n",
       "      <td>7.268591</td>\n",
       "      <td>7.342052</td>\n",
       "      <td>7.424016</td>\n",
       "      <td>7.407117</td>\n",
       "      <td>7.399234</td>\n",
       "      <td>7.420797</td>\n",
       "      <td>7.484445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g_9a665aae6d</td>\n",
       "      <td>0.039626</td>\n",
       "      <td>0.079531</td>\n",
       "      <td>0.119691</td>\n",
       "      <td>0.158211</td>\n",
       "      <td>0.198463</td>\n",
       "      <td>0.237721</td>\n",
       "      <td>0.277626</td>\n",
       "      <td>0.315779</td>\n",
       "      <td>0.356901</td>\n",
       "      <td>...</td>\n",
       "      <td>10.013875</td>\n",
       "      <td>10.048609</td>\n",
       "      <td>10.039421</td>\n",
       "      <td>10.076058</td>\n",
       "      <td>10.135363</td>\n",
       "      <td>10.206509</td>\n",
       "      <td>10.220245</td>\n",
       "      <td>10.213495</td>\n",
       "      <td>10.247083</td>\n",
       "      <td>10.356003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g_ba4abe1b9e</td>\n",
       "      <td>0.019238</td>\n",
       "      <td>0.039122</td>\n",
       "      <td>0.059521</td>\n",
       "      <td>0.076708</td>\n",
       "      <td>0.097912</td>\n",
       "      <td>0.116377</td>\n",
       "      <td>0.135869</td>\n",
       "      <td>0.152322</td>\n",
       "      <td>0.174366</td>\n",
       "      <td>...</td>\n",
       "      <td>5.051709</td>\n",
       "      <td>5.045511</td>\n",
       "      <td>4.957505</td>\n",
       "      <td>5.036664</td>\n",
       "      <td>5.160219</td>\n",
       "      <td>5.224001</td>\n",
       "      <td>5.184262</td>\n",
       "      <td>5.160882</td>\n",
       "      <td>5.156343</td>\n",
       "      <td>5.232307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     geology_id         1         2         3         4         5         6  \\\n",
       "0  g_4a52df537a -0.006917 -0.013030 -0.019454 -0.027568 -0.032346 -0.039223   \n",
       "1  g_1e4b5a1509  0.013678  0.028200  0.042944  0.054485  0.070491  0.083501   \n",
       "2  g_5919c0bea3  0.028208  0.056853  0.085918  0.112302  0.142245  0.169681   \n",
       "3  g_9a665aae6d  0.039626  0.079531  0.119691  0.158211  0.198463  0.237721   \n",
       "4  g_ba4abe1b9e  0.019238  0.039122  0.059521  0.076708  0.097912  0.116377   \n",
       "\n",
       "          7         8         9  ...  r_9_pos_291  r_9_pos_292  r_9_pos_293  \\\n",
       "0 -0.046534 -0.056739 -0.058770  ...    -1.441678    -1.524261    -1.582406   \n",
       "1  0.097020  0.107700  0.125039  ...     3.692461     3.666321     3.558129   \n",
       "2  0.198137  0.224085  0.254682  ...     7.240481     7.264407     7.187561   \n",
       "3  0.277626  0.315779  0.356901  ...    10.013875    10.048609    10.039421   \n",
       "4  0.135869  0.152322  0.174366  ...     5.051709     5.045511     4.957505   \n",
       "\n",
       "   r_9_pos_294  r_9_pos_295  r_9_pos_296  r_9_pos_297  r_9_pos_298  \\\n",
       "0    -1.557901    -1.414614    -1.386626    -1.417733    -1.498469   \n",
       "1     3.647701     3.779645     3.833497     3.800299     3.756183   \n",
       "2     7.268591     7.342052     7.424016     7.407117     7.399234   \n",
       "3    10.076058    10.135363    10.206509    10.220245    10.213495   \n",
       "4     5.036664     5.160219     5.224001     5.184262     5.160882   \n",
       "\n",
       "   r_9_pos_299  r_9_pos_300  \n",
       "0    -1.480004    -1.477566  \n",
       "1     3.747805     3.832135  \n",
       "2     7.420797     7.484445  \n",
       "3    10.247083    10.356003  \n",
       "4     5.156343     5.232307  \n",
       "\n",
       "[5 rows x 3001 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 7: ‚ÄúAverage Trick‚Äù Post‚ÄêProcessing\n",
    "df_sub = sub_df.copy()\n",
    "numeric_values = df_sub.iloc[:, 1:].values\n",
    "n_samples = numeric_values.shape[0]\n",
    "data_reshaped = numeric_values.reshape(n_samples, 10, 300)\n",
    "mean_across_realizations = data_reshaped.mean(axis=1)\n",
    "mean_repeated = np.tile(mean_across_realizations[:, None, :], (1, 10, 1))\n",
    "mean_repeated = mean_repeated.reshape(n_samples, 3000)\n",
    "df_sub.iloc[:, 1:] = mean_repeated\n",
    "\n",
    "df_sub.to_csv(\"submission_refined.csv\", index=False)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc4578-f9c7-4bac-9d5a-15899b7269e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
